{
  "turn": 1,
  "id": "483289415",
  "url_id": "real-time-narrative-risk-detection-system",
  "description": "Real-Time Narrative Risk Detection System",
  "generated_app_id": "33e9bb7e-4815-40b8-8bbd-4443fc49cf2e",
  "project_id": "2288",
  "messages": [
    {
      "role": "user",
      "parts": [
        {
          "type": "text",
          "text": "\n\nBuild a fullstack hackathon project called 'Real-Time Narrative Risk Detection System'.\n\nThis project consists of two parts:\n1. A FastAPI Python backend with NLP analysis, web scraping, and external API integration\n2. A React + Vite + Tailwind CSS frontend dashboard\n\n---\n\n## USER INPUT MODES\n\nThe system accepts two types of input:\n\n### Mode 1: URL Input\n- User provides a website URL\n- Backend scrapes content from the URL using BeautifulSoup first; if that fails (JS-heavy site), fall back to Playwright\n- Also validate the website's credibility/trustworthiness (check domain reputation, HTTPS, known trusted domains list)\n- Analyze the scraped content for sentiment, coordination, and risk\n\n### Mode 2: Text/Topic Input\n- User provides a sentence or topic keyword\n- Backend searches the web using:\na. Google Search via Serper.dev API â for general web content\nb. NewsAPI â for trusted news articles\n- Filter results to authorized/reputable sources only\n- Scrape and aggregate content from the top results\n- Analyze the aggregated content for sentiment, coordination, and risk\n\nThe backend should auto-detect whether the input is a URL or a text/topic.\n\n---\n\n## BACKEND (FastAPI)\n\n### Endpoint\nPOST /analyze\n- Accepts JSON: { \"input\": \"string\" } (can be a URL or a topic/sentence)\n- Returns full analysis JSON\n\n### Processing Pipeline:\n\n1. **Input Detection**: Check if input is a valid URL or plain text/topic.\n\n2. **Content Fetching**:\n- If URL: Scrape using BeautifulSoup (requests + bs4). If scraping fails or returns empty, fall back to Playwright.\n- If text/topic: Call Serper.dev API (Google Search) and NewsAPI to fetch top results. Scrape content from top 5 URLs returned.\n\n3. **Website Credibility Validation** (for URL input or fetched source URLs):\n- Check if domain uses HTTPS\n- Check against a hardcoded list of known trusted domains (e.g., bbc.com, reuters.com, nytimes.com, wikipedia.org, etc.)\n- Assign a source_trust_score (0.0 to 1.0) based on these checks\n\n4. **Sentiment Analysis using VADER**:\n- Split content into sentences/chunks\n- Run each through VADER SentimentIntensityAnalyzer\n- Compute % positive, neutral, negative\n- Return as: { \"positive\": int, \"neutral\": int, \"negative\": int }\n\n5. **Coordination Detection using TF-IDF + Cosine Similarity**:\n- Vectorize all text chunks using TfidfVectorizer\n- Compute pairwise cosine similarity\n- Count pairs with similarity > 0.8\n- Compute average similarity_score (0.0 to 1.0)\n\n6. **Risk Scoring**:\n- HIGH if: negative > 50% OR similarity_score > 0.6 OR source_trust_score < 0.3\n- MEDIUM if: negative > 30% OR similarity_score > 0.4 OR source_trust_score < 0.6\n- LOW otherwise\n- Populate reasons[] with human-readable explanations\n\n7. **Related Information** (using Serper.dev + NewsAPI):\n- Related articles: Top 3-5 news articles related to the topic/URL content\n- Fact-check results: Search for fact-check pages related to the topic (e.g., snopes, factcheck.org, politifact)\n- Key entities: Extract named entities (people, organizations, locations) from the content using basic NLP (spaCy or regex-based NER)\n\n8. **Response format**:\n```json\n{\n\"input_type\": \"url | text\",\n\"source_trust_score\": 0.0,\n\"sentiment\": {\n\"positive\": 0,\n\"neutral\": 0,\n\"negative\": 0\n},\n\"similarity_score\": 0.0,\n\"risk_level\": \"LOW | MEDIUM | HIGH\",\n\"reasons\": [],\n\"related\": {\n\"articles\": [\n{ \"title\": \"\", \"url\": \"\", \"source\": \"\" }\n],\n\"fact_checks\": [\n{ \"title\": \"\", \"url\": \"\", \"source\": \"\" }\n],\n\"entities\": [\n{ \"name\": \"\", \"type\": \"PERSON | ORG | LOCATION\" }\n]\n}\n}\n```\n\n### CORS\n- Enable CORS for all origins\n\n### Dependencies\n- fastapi, uvicorn\n- vaderSentiment\n- scikit-learn, numpy\n- requests, beautifulsoup4\n- playwright (async, as fallback scraper)\n- spacy (for NER, use en_core_web_sm model)\n- python-dotenv (for API keys)\n\n### API Keys (stored in .env)\n- SERPER_API_KEY â for Serper.dev Google Search API\n- NEWS_API_KEY â for NewsAPI\n\n---\n\n## FRONTEND (React + Vite + Tailwind CSS)\n\n- Dark theme, clean and modern UI (bg-gray-900, text-white)\n- Single input field that accepts both URLs and text/topic (auto-detected by backend)\n- Analyze button â sends POST to http://localhost:8000/analyze with { input: string }\n- Loading state while API call is in progress\n\n### Results Display:\n1. **Risk Alert Box** â color-coded: green (LOW), yellow (MEDIUM), red (HIGH)\n2. **Source Trust Score** â displayed as a progress bar or badge (0.0 to 1.0)\n3. **Sentiment Pie Chart** â Recharts PieChart with positive/neutral/negative breakdown\n4. **Similarity Score** â displayed as a numeric badge or progress bar\n5. **Reasons List** â styled list of human-readable risk reasons\n6. **Related Articles** â card list with title, source, and clickable URL\n7. **Fact-Check Results** â card list with title, source, and clickable URL\n8. **Key Entities** â tag/badge list showing people, organizations, locations\n\n- Use Axios for API calls\n- Handle errors gracefully\n\n### Dependencies\n- axios, recharts, tailwindcss\n\n---\n\n## Implementation Steps for AI Engineer\n\n1. Set up FastAPI backend:\na. main.py with CORS, /analyze endpoint\nb. input_detector.py â detect URL vs text\nc. scraper.py â BeautifulSoup scraper with Playwright fallback\nd. credibility.py â domain trust scoring\ne. sentiment.py â VADER analysis\nf. coordination.py â TF-IDF + cosine similarity\ng. risk.py â risk scoring logic\nh. related.py â fetch related articles, fact-checks, entities via Serper + NewsAPI + spaCy\ni. .env file with SERPER_API_KEY and NEWS_API_KEY\n\n2. Set up React + Vite frontend:\na. Scaffold with Vite, configure Tailwind\nb. Install axios, recharts\nc. Build Dashboard with single input, analyze button, loading state\nd. Render all result sections on response\ne. Handle errors\n\n3. Run:\n- Backend: uvicorn main:app --reload (port 8000)\n- Frontend: vite dev (port 5173)\n\n---\n\nThe user provided the following instructions above. You are an AI assistant that helps users solve problems using code skills, and a powerful data pipeline API system. This system allows you to ingest data from multiple sources, process it with custom prompts, and create derived data objects for complex workflows.\n\nGenerated App ID: 33e9bb7e-4815-40b8-8bbd-4443fc49cf2e\n\nYour task:\n1. The user has put together a requirements doc and wants your help putting functions together to create an app that does some task and then returns results to them. This may or may not require the API system. Help the user no matter what they need.\n\n## Your Role\n\nAs an AI assistant using this system:\n1. **Design efficient pipelines** that minimize API calls while maximizing insight\n2. **Provide clear API calls** with proper JSON formatting\n3. **Explain your reasoning** for the chosen approach\n4. **Suggest follow-up steps** or alternative approaches when helpful\n\n\n## Framework\n\nIMPORTANT: Build this project using fastapi. Follow fastapi best practices and conventions.\n\n## Base API Templates\n\n## Available API Endpoints (all API endpoints are at https://builder-api.empromptu.ai)\n\n**CRITICAL - Required Authentication Headers for ALL API calls:**\n```javascript\nconst API_BASE_URL = 'https://builder-api.empromptu.ai';\nconst AUTH_HEADERS = {\n'Content-Type': 'application/json',\n'Authorization': 'Bearer ce7832cb03869ddc82a660a439b57eda',\n'X-Generated-App-ID': '33e9bb7e-4815-40b8-8bbd-4443fc49cf2e',\n'X-Usage-Key': '5770ffa4b704f546ed780345ae055193'\n};\n\n// Example fetch call:\nconst response = await fetch(`${API_BASE_URL}/apply_prompt_to_data`, {\nmethod: 'POST',\nheaders: AUTH_HEADERS,\nbody: JSON.stringify(data)\n});\n```\n\nWithout these headers, API calls will fail with 401 Unauthorized.\n\n### 1. AI prompt setup: '/setup_ai_prompt'\ninput_variables: List[str]\nprompt_text: str\n\n**Purpose**: Set up an AI prompt to be applied to text-only data later on.\n**Method**: POST\n**Parameters**:\n- \"prompt_name\" (string): Name of the new prompt.  Should be short and the same each time it is called.\n- \"input_variables\" (list): List of strings. Each of these will be included in the prompt text.\n- \"prompt_text\" (string): Text of the new prompt. Only one unique string should ever be associated with each prompt_name.  Each input variable should be included in this text, in curly braces.\n**Example Usage**:\nRequest body:\n{\n\"prompt_name\": \"summarize_info\",\n\"input_variables\": [\"num_points\",\"raw_info\"],\n\"prompt_text\": \"Please summarize the following data using {num_points} bullet points: {raw_info}\"\n}\n\n### 2. Data Processing: '/apply_prompt_to_data'\n**Purpose**: Apply AI prompts to data to generate new insights\n**Method**: POST\n**Parameters**:\n- 'prompt_name' (string): Name of the prompt to use\n- 'input_data' (object): Names of new objects to create from results. Should be short and the same each time it is called.\n- 'return_type' (string): Format to retrieve object in. The actual data will be in a key called \"value\" returned from this call.\n**Object Formats for return_type**:\nThese determine how the data in the key called 'value' will be formatted.  Options:\n- 'pretty_text': Text formatted for human reading. Should be used where easily readable text is desired.\n- 'pretty_html': Data formatted as an array of HTML strings (each line of the HTML document is a separate array element), to be read by a browser or injected into a web page.\n- 'raw_text': Merge all data from the object into one combined text. Only suitable for additional analysis or parsing.\n- 'structured': Returns a native JavaScript array of objects, like [{\"key1\":\"val1\",\"key2\":\"val2\"},{...}]. Designed to use when the answer has multiple parts or elements. If you use this be sure to write a robust data handler that can handle format variations.\n\n**Example Usage**:\nRequest body:\n{\n\"prompt_name\": \"summarize_info\",\n\"input_data\": {\n\"num_points\": \"5\",\n\"raw_info\": \"Here is some information about a topic of interest......\",\n\"return_type\": \"pretty_text\"\n}\n}\nHeaders must include Authorization and X-Generated-App-ID as shown above.\n\n### 3. Agent Creation\n\nMake a new agent: /create-agent\n**Purpose**: Create a new chatbot to handle a specific question or problem\n**Method**: POST\n**Parameters**:\n- 'instructions' (string): Brief description of what the chatbot or agent should accomplish.\n- 'agent_name' (string): What this agent calls itself in conversation.\n**Returned Values**\n- 'agent_id' (string): ID for using this agent.\n\n**Example Usage**:\n{\n\"instructions\": \"\"\"You are a professional data processing assistant for Impromptu Labs.\nYou help users store, manage, and process data using our MongoDB system.\"\"\",\n\"agent_name\": \"Data Processing Assistant\"\n}\n\nTalk to an existing agent: /chat\n**Purpose**: Continue the conversation with an agent\n**Method**: POST\n**Parameters**:\n- 'agent_id' (string): The system name for the agent, returned from create-agent\n- 'message' (string): Input text to send to the agent.\n- 'session_id' (string): ID string representing this session. Use a new one for each conversation.\n**Returned Values**\n- 'response' (string): The text response from the agent.\n\n**Example Usage**:\n{\n\"agent_id\": agent_id,\n\"message\": \"Hi! I'm new to this system. Can you help me understand how to store data?\"\n}\n\n### 4. Image Analysis\nAnalyze the contents of an Image: /analyze_image\n**Purpose**: Do all extraction of information from images in one step.  This is a standalone step to go from image to text.  It requires image data as input, so extract images from documents as necessary before using this method.\n**Method**: POST\n**Parameters**:\n- 'image' (binary): Binary image file data. Note that you need to send the data as multipart form data with an actual file, not as JSON with a base64 string. Additionally, the API expects an image field, not \"file.\"\n- 'prompt' (string): A prompt to analyze the image, e.g. \"Descibe the contents of this image\" or \"how many dogs are in this image\".\n**Returned Values**\n- 'value' (string): Text response from the image analyzer.\n\n### 5. URL/Webpage Reading: /get_data_from_url\ninput_data (string): URL to retrieve the text from.\n**Purpose**: Extract text from a web page\n**Method**: POST\n**Parameters**:\ninput_data (string): URL to retrieve the text from.\n**Returned Values**\n- 'text' (string): Text from the web page.\n\n## General Problem-Solving Approach to using this API:\nWhen a user presents a problem, follow this systematic approach:\n\n### Step 1: Analyze the Problem\n- What data sources are involved? (text, files, websites, etc.)\n- What processing or analysis is needed?\n- What output format or insights are desired?\n- Are there multiple steps or transformations required?\n\n### Step 2: Plan the Pipeline\n1. **Data Ingestion**: Identify what needs to be imported and how\n2. **Processing Steps**: Determine what prompts/transformations are needed\n3. **Output Goals**: Define what final objects should be created\n\n### Step 3: Execute the Solution\n- Set up the AI prompts using '/setup_ai_prompt'\n- Apply the AI prompts to input text using '/apply_prompt_to_data'\n- Chain multiple processing steps if needed\n- Verify results using the management endpoints\n\n## Advanced Patterns\n\n### Iterative Processing\nBuild complex workflows by chaining operations:\n1. Ingest raw data\n2. Extract key points\n3. Categorize points\n4. Generate final report\n\n## Example Problem-Solving Scenarios\n\n### Competitive Intelligence\n**User**: \"Compare our product features with 5 competitor websites\"\n**Solution**:\n1. Use '/get_data_from_url' to scrape competitor sites\n2. Use '/setup_ai_prompt' to prepare a prompt that extracts product info\n3. Use '/apply_prompt_to_data' to extract features from each web page's data\n4. Compare the resulting data\n\n### Document Processing\n**User**: \"Extract action items from 20 meeting transcripts and categorize them\"\n**Solution**:\n1. Upload transcripts and process them locally\n2. Use '/setup_ai_prompt' to prepare a prompt that extracts action items\n3. Use '/setup_ai_prompt' to prepare a prompt that categorizes action items\n3. Use '/apply_prompt_to_data' with 'structured' return_type\n4. Iterate through each output and use '/apply_prompt_to_data' to categorize each item\n\n## Best Practices\n\n### Naming Conventions\n- Use descriptive object names: 'meeting_transcripts', 'competitor_analysis'\n- Include processing step in names: 'raw_articles' â 'article_summaries' â 'final_report'\n\n### Prompt Engineering\n- Use clear placeholders: '{object_name}'\n- Anywhere an object name is needed, use a single consistent name instead of appending UIDs or timestamps.\n- Specify desired output format in prompts\n- Include example outputs when helpful\n- Request structured data when building pipelines\n\n### Efficiency\n- Use existing capabilities as they are rather than rewriting them\n- All error messages displayed in the app should include the direct output from the API function being called.\n- Ensure you set up any AI prompts before you use them.\n- If PDF parsing is necessary, add it using the client-side library pdfjs-dist, and the line: import pdfjsWorker from 'pdfjs-dist/build/pdf.worker?url'\n- If plots and charts are necessary, add them using Chart.js with react-chartjs-2.\n- Combine related processing steps when possible\n\n## Additional functions and APIs:\n\n\n## Running the app\n\nGive the code one more quick check.  If you have not done so yet, run the necessary commands by calling the `shell_exec` tool (e.g., install dependencies with `npm install`, run tests, or invoke curl to exercise the builder APIs). When you need to create or update files, call `fs_write` with the full file content.always use the registered tools for filesystem and shell access.\n\nAlso, create any text-processing prompts that you will need by issuing curl calls (via `shell_exec` + curl) to the `/setup_ai_prompt` endpoint with the prompts this app will use. Provide the relevant commands and summarize the outputs so the user knows the APIs were exercised.\n\nFrom here onward, ensure that every code or command change goes through the appropriate tool call so it actually takes effect.\n\n## Mocking Third-Party APIs\n\nFor all third-party APIs that are mentioned in the user instructions and are not listed in the API endpoints provided, please mock them. For example, if the user mentioned integrating a third party such as ElevenLabs or Fish AI, and there are no explicit instructions on how to integrate them, mock them using reasonable assumptions of the API structure.\n\n## Design Requirements\n\nIf the user requirements did not define styles, change the style of the app using the set of instructions below that are most relevant to the user task:\n\n(For screens where users upload documents, extract structured data, and view outputs):\nGenerate a three-step Upload & Extract flow for seed-to-Series-B small-business brands.\nâ¢ Step 1: File upload card with drag-and-drop and âChoose Fileâ button, branded with some PRIMARY_COLOR of your choice.\nâ¢ Step 2: Extraction progress screen showing a Tailwind-styled spinner, clear status message, and cancel option.\nâ¢ Step 3: Data output table with Tailwind table classes, sortable columns, and âDownload CSVâ button.\nâ¢ Step 4: Ensure mobile breakpoints collapse sidebars into accordion panels; implement dark mode variants; include ARIA labels on all interactive elements.â\n\n(For embedding a conversational AI widget into an existing portal):\nProduce a chatbot UI panel that sits at bottom-right:\nâ¢ Step 1:  Minimal header bar with logo icon and âHelpâ label in some PRIMARY_COLOR of your choice.\nâ¢ Step 2:  Scrollable message window styled with alternating light/dark bubble backgrounds.\nâ¢ Step 3:  Input area with accessible placeholder text 'Ask me anything...', a send icon button, and an optional attachment button.\nâ¢ Step 4:  Ensure focus outlines, keyboard navigation, and proper aria-live regions for new messages.\nâ¢ Step 5:  Include a mobile view where the panel expands full-screen and a dark-mode toggle.â\n\n(For workflows where users upload a document, enter a prompt themselves, then see a summary or output):\nDesign a three-column layout on desktop, single-column on mobile, for Upload + Prompt + Summary:\nâ¢ Step 1:  Upload Column: Drag-and-drop zone with dashed border and an upload progress bar.\nâ¢ Step 2:  Prompt Column: A text area with placeholder 'Enter instructions...', character count, and 'Run' button in green.\nâ¢ Step 3:  Summary Column: Shows AI-generated summary in a scrollable card with expandable sections.\nâ¢ Step 4:  Follow minimal luxe style: plenty of white space, 2xl rounded corners, soft shadows. Use Tailwind classes for spacing. Include dark-mode card variants. Add aria-describedby on summary sections.â\n\n## Secrets Vault Requirements\n\n\nSECRET VAULT CONNECTION INFORMATION\n==============================\nYou already have access to a server-side encrypted storage for API keys, passwords, and other secrets. Secrets persist between server restarts and are never exposed to the browser. Use this to hold user secrets or other keys.\n\nThis is different from the API calls - You already wrote the underlying code, and the files are presented as truncated for the sake of space.  Do not attempt to rewrite these files or re-create them elsewhere..\n\nTo use this secrets vault, all you have to do is the following:\ncd server\nnpm install express\n\nThen, add this import to your code and use the following local function structure:\n\nimport { useSecrets } from './hooks/useSecrets';\n\nfunction MyComponent() {\n  const { getSecret, setSecret } = useSecrets();\n\n  // Get a secret\n  const var1 = await getSecret('my_variable_1');\n\n  // Set a secret\n  await setSecret('database_password', 'my-password');\n\n  return <div>Variable 1: {var1}</div>;\n}\n\nSecrets set via this method will persist and can be used across runs of the app. Just use the existing files and call the vault this way.\n\n"
        }
      ],
      "metadata": {
        "isFirstBuild": true,
        "features": {
          "fastapi_backend": true,
          "post_analyze_endpoint": true,
          "url_input_mode": true,
          "text_topic_input_mode": true,
          "auto_input_detection": true,
          "web_scraping_beautifulsoup": true,
          "web_scraping_playwright_fallback": true,
          "website_credibility_validation": true,
          "source_trust_score": true,
          "vader_sentiment_analysis": true,
          "tfidf_cosine_similarity": true,
          "risk_scoring_logic": true,
          "related_articles_display": true,
          "fact_check_results": true,
          "named_entity_extraction": true,
          "cors_enabled": true,
          "dark_theme_ui": true,
          "topic_input": true,
          "analyze_button": true,
          "risk_level_alert": true,
          "sentiment_pie_chart": true,
          "similarity_score_display": true,
          "risk_reasons_section": true,
          "axios_api_integration": true,
          "recharts_visualization": true,
          "loading_state": true,
          "error_handling": true,
          "secrets_vault": true,
          "usesFigma": false,
          "framework": "fastapi",
          "projectType": "fullstack"
        },
        "hasTemplateFiles": true
      },
      "id": "k8gPeBhacdPeltZB"
    }
  ],
  "timestamp": "2026-02-27T09:17:26.113676Z",
  "user_id": "ce7832cb03869ddc82a660a439b57eda",
  "latest_user_statement": "\n\nBuild a fullstack hackathon project called 'Real-Time Narrative Risk Detection System'.\n\nThis project consists of two parts:\n1. A FastAPI Python backend with NLP analysis, web scraping, and external API integration\n2. A React + Vite + Tailwind CSS frontend dashboard\n\n---\n\n## USER INPUT MODES\n\nThe system accepts two types of input:\n\n### Mode 1: URL Input\n- User provides a website URL\n- Backend scrapes content from the URL using BeautifulSoup first; if that fails (JS-heavy site), fall back to Playwright\n- Also validate the website's credibility/trustworthiness (check domain reputation, HTTPS, known trusted domains list)\n- Analyze the scraped content for sentiment, coordination, and risk\n\n### Mode 2: Text/Topic Input\n- User provides a sentence or topic keyword\n- Backend searches the web using:\na. Google Search via Serper.dev API â for general web content\nb. NewsAPI â for trusted news articles\n- Filter results to authorized/reputable sources only\n- Scrape and aggregate content from the top results\n- Analyze the aggregated content for sentiment, coordination, and risk\n\nThe backend should auto-detect whether the input is a URL or a text/topic.\n\n---\n\n## BACKEND (FastAPI)\n\n### Endpoint\nPOST /analyze\n- Accepts JSON: { \"input\": \"string\" } (can be a URL or a topic/sentence)\n- Returns full analysis JSON\n\n### Processing Pipeline:\n\n1. **Input Detection**: Check if input is a valid URL or plain text/topic.\n\n2. **Content Fetching**:\n- If URL: Scrape using BeautifulSoup (requests + bs4). If scraping fails or returns empty, fall back to Playwright.\n- If text/topic: Call Serper.dev API (Google Search) and NewsAPI to fetch top results. Scrape content from top 5 URLs returned.\n\n3. **Website Credibility Validation** (for URL input or fetched source URLs):\n- Check if domain uses HTTPS\n- Check against a hardcoded list of known trusted domains (e.g., bbc.com, reuters.com, nytimes.com, wikipedia.org, etc.)\n- Assign a source_trust_score (0.0 to 1.0) based on these checks\n\n4. **Sentiment Analysis using VADER**:\n- Split content into sentences/chunks\n- Run each through VADER SentimentIntensityAnalyzer\n- Compute % positive, neutral, negative\n- Return as: { \"positive\": int, \"neutral\": int, \"negative\": int }\n\n5. **Coordination Detection using TF-IDF + Cosine Similarity**:\n- Vectorize all text chunks using TfidfVectorizer\n- Compute pairwise cosine similarity\n- Count pairs with similarity > 0.8\n- Compute average similarity_score (0.0 to 1.0)\n\n6. **Risk Scoring**:\n- HIGH if: negative > 50% OR similarity_score > 0.6 OR source_trust_score < 0.3\n- MEDIUM if: negative > 30% OR similarity_score > 0.4 OR source_trust_score < 0.6\n- LOW otherwise\n- Populate reasons[] with human-readable explanations\n\n7. **Related Information** (using Serper.dev + NewsAPI):\n- Related articles: Top 3-5 news articles related to the topic/URL content\n- Fact-check results: Search for fact-check pages related to the topic (e.g., snopes, factcheck.org, politifact)\n- Key entities: Extract named entities (people, organizations, locations) from the content using basic NLP (spaCy or regex-based NER)\n\n8. **Response format**:\n```json\n{\n\"input_type\": \"url | text\",\n\"source_trust_score\": 0.0,\n\"sentiment\": {\n\"positive\": 0,\n\"neutral\": 0,\n\"negative\": 0\n},\n\"similarity_score\": 0.0,\n\"risk_level\": \"LOW | MEDIUM | HIGH\",\n\"reasons\": [],\n\"related\": {\n\"articles\": [\n{ \"title\": \"\", \"url\": \"\", \"source\": \"\" }\n],\n\"fact_checks\": [\n{ \"title\": \"\", \"url\": \"\", \"source\": \"\" }\n],\n\"entities\": [\n{ \"name\": \"\", \"type\": \"PERSON | ORG | LOCATION\" }\n]\n}\n}\n```\n\n### CORS\n- Enable CORS for all origins\n\n### Dependencies\n- fastapi, uvicorn\n- vaderSentiment\n- scikit-learn, numpy\n- requests, beautifulsoup4\n- playwright (async, as fallback scraper)\n- spacy (for NER, use en_core_web_sm model)\n- python-dotenv (for API keys)\n\n### API Keys (stored in .env)\n- SERPER_API_KEY â for Serper.dev Google Search API\n- NEWS_API_KEY â for NewsAPI\n\n---\n\n## FRONTEND (React + Vite + Tailwind CSS)\n\n- Dark theme, clean and modern UI (bg-gray-900, text-white)\n- Single input field that accepts both URLs and text/topic (auto-detected by backend)\n- Analyze button â sends POST to http://localhost:8000/analyze with { input: string }\n- Loading state while API call is in progress\n\n### Results Display:\n1. **Risk Alert Box** â color-coded: green (LOW), yellow (MEDIUM), red (HIGH)\n2. **Source Trust Score** â displayed as a progress bar or badge (0.0 to 1.0)\n3. **Sentiment Pie Chart** â Recharts PieChart with positive/neutral/negative breakdown\n4. **Similarity Score** â displayed as a numeric badge or progress bar\n5. **Reasons List** â styled list of human-readable risk reasons\n6. **Related Articles** â card list with title, source, and clickable URL\n7. **Fact-Check Results** â card list with title, source, and clickable URL\n8. **Key Entities** â tag/badge list showing people, organizations, locations\n\n- Use Axios for API calls\n- Handle errors gracefully\n\n### Dependencies\n- axios, recharts, tailwindcss\n\n---\n\n## Implementation Steps for AI Engineer\n\n1. Set up FastAPI backend:\na. main.py with CORS, /analyze endpoint\nb. input_detector.py â detect URL vs text\nc. scraper.py â BeautifulSoup scraper with Playwright fallback\nd. credibility.py â domain trust scoring\ne. sentiment.py â VADER analysis\nf. coordination.py â TF-IDF + cosine similarity\ng. risk.py â risk scoring logic\nh. related.py â fetch related articles, fact-checks, entities via Serper + NewsAPI + spaCy\ni. .env file with SERPER_API_KEY and NEWS_API_KEY\n\n2. Set up React + Vite frontend:\na. Scaffold with Vite, configure Tailwind\nb. Install axios, recharts\nc. Build Dashboard with single input, analyze button, loading state\nd. Render all result sections on response\ne. Handle errors\n\n3. Run:\n- Backend: uvicorn main:app --reload (port 8000)\n- Frontend: vite dev (port 5173)\n\n---\n\nThe user provided the following instructions above. You are an AI assistant that helps users solve problems using code skills, and a powerful data pipeline API system. This system allows you to ingest data from multiple sources, process it with custom prompts, and create derived data objects for complex workflows.\n\nGenerated App ID: 33e9bb7e-4815-40b8-8bbd-4443fc49cf2e\n\nYour task:\n1. The user has put together a requirements doc and wants your help putting functions together to create an app that does some task and then returns results to them. This may or may not require the API system. Help the user no matter what they need.\n\n## Your Role\n\nAs an AI assistant using this system:\n1. **Design efficient pipelines** that minimize API calls while maximizing insight\n2. **Provide clear API calls** with proper JSON formatting\n3. **Explain your reasoning** for the chosen approach\n4. **Suggest follow-up steps** or alternative approaches when helpful\n\n\n## Framework\n\nIMPORTANT: Build this project using fastapi. Follow fastapi best practices and conventions.\n\n## Base API Templates\n\n## Available API Endpoints (all API endpoints are at https://builder-api.empromptu.ai)\n\n**CRITICAL - Required Authentication Headers for ALL API calls:**\n```javascript\nconst API_BASE_URL = 'https://builder-api.empromptu.ai';\nconst AUTH_HEADERS = {\n'Content-Type': 'application/json',\n'Authorization': 'Bearer ce7832cb03869ddc82a660a439b57eda',\n'X-Generated-App-ID': '33e9bb7e-4815-40b8-8bbd-4443fc49cf2e',\n'X-Usage-Key': '5770ffa4b704f546ed780345ae055193'\n};\n\n// Example fetch call:\nconst response = await fetch(`${API_BASE_URL}/apply_prompt_to_data`, {\nmethod: 'POST',\nheaders: AUTH_HEADERS,\nbody: JSON.stringify(data)\n});\n```\n\nWithout these headers, API calls will fail with 401 Unauthorized.\n\n### 1. AI prompt setup: '/setup_ai_prompt'\ninput_variables: List[str]\nprompt_text: str\n\n**Purpose**: Set up an AI prompt to be applied to text-only data later on.\n**Method**: POST\n**Parameters**:\n- \"prompt_name\" (string): Name of the new prompt.  Should be short and the same each time it is called.\n- \"input_variables\" (list): List of strings. Each of these will be included in the prompt text.\n- \"prompt_text\" (string): Text of the new prompt. Only one unique string should ever be associated with each prompt_name.  Each input variable should be included in this text, in curly braces.\n**Example Usage**:\nRequest body:\n{\n\"prompt_name\": \"summarize_info\",\n\"input_variables\": [\"num_points\",\"raw_info\"],\n\"prompt_text\": \"Please summarize the following data using {num_points} bullet points: {raw_info}\"\n}\n\n### 2. Data Processing: '/apply_prompt_to_data'\n**Purpose**: Apply AI prompts to data to generate new insights\n**Method**: POST\n**Parameters**:\n- 'prompt_name' (string): Name of the prompt to use\n- 'input_data' (object): Names of new objects to create from results. Should be short and the same each time it is called.\n- 'return_type' (string): Format to retrieve object in. The actual data will be in a key called \"value\" returned from this call.\n**Object Formats for return_type**:\nThese determine how the data in the key called 'value' will be formatted.  Options:\n- 'pretty_text': Text formatted for human reading. Should be used where easily readable text is desired.\n- 'pretty_html': Data formatted as an array of HTML strings (each line of the HTML document is a separate array element), to be read by a browser or injected into a web page.\n- 'raw_text': Merge all data from the object into one combined text. Only suitable for additional analysis or parsing.\n- 'structured': Returns a native JavaScript array of objects, like [{\"key1\":\"val1\",\"key2\":\"val2\"},{...}]. Designed to use when the answer has multiple parts or elements. If you use this be sure to write a robust data handler that can handle format variations.\n\n**Example Usage**:\nRequest body:\n{\n\"prompt_name\": \"summarize_info\",\n\"input_data\": {\n\"num_points\": \"5\",\n\"raw_info\": \"Here is some information about a topic of interest......\",\n\"return_type\": \"pretty_text\"\n}\n}\nHeaders must include Authorization and X-Generated-App-ID as shown above.\n\n### 3. Agent Creation\n\nMake a new agent: /create-agent\n**Purpose**: Create a new chatbot to handle a specific question or problem\n**Method**: POST\n**Parameters**:\n- 'instructions' (string): Brief description of what the chatbot or agent should accomplish.\n- 'agent_name' (string): What this agent calls itself in conversation.\n**Returned Values**\n- 'agent_id' (string): ID for using this agent.\n\n**Example Usage**:\n{\n\"instructions\": \"\"\"You are a professional data processing assistant for Impromptu Labs.\nYou help users store, manage, and process data using our MongoDB system.\"\"\",\n\"agent_name\": \"Data Processing Assistant\"\n}\n\nTalk to an existing agent: /chat\n**Purpose**: Continue the conversation with an agent\n**Method**: POST\n**Parameters**:\n- 'agent_id' (string): The system name for the agent, returned from create-agent\n- 'message' (string): Input text to send to the agent.\n- 'session_id' (string): ID string representing this session. Use a new one for each conversation.\n**Returned Values**\n- 'response' (string): The text response from the agent.\n\n**Example Usage**:\n{\n\"agent_id\": agent_id,\n\"message\": \"Hi! I'm new to this system. Can you help me understand how to store data?\"\n}\n\n### 4. Image Analysis\nAnalyze the contents of an Image: /analyze_image\n**Purpose**: Do all extraction of information from images in one step.  This is a standalone step to go from image to text.  It requires image data as input, so extract images from documents as necessary before using this method.\n**Method**: POST\n**Parameters**:\n- 'image' (binary): Binary image file data. Note that you need to send the data as multipart form data with an actual file, not as JSON with a base64 string. Additionally, the API expects an image field, not \"file.\"\n- 'prompt' (string): A prompt to analyze the image, e.g. \"Descibe the contents of this image\" or \"how many dogs are in this image\".\n**Returned Values**\n- 'value' (string): Text response from the image analyzer.\n\n### 5. URL/Webpage Reading: /get_data_from_url\ninput_data (string): URL to retrieve the text from.\n**Purpose**: Extract text from a web page\n**Method**: POST\n**Parameters**:\ninput_data (string): URL to retrieve the text from.\n**Returned Values**\n- 'text' (string): Text from the web page.\n\n## General Problem-Solving Approach to using this API:\nWhen a user presents a problem, follow this systematic approach:\n\n### Step 1: Analyze the Problem\n- What data sources are involved? (text, files, websites, etc.)\n- What processing or analysis is needed?\n- What output format or insights are desired?\n- Are there multiple steps or transformations required?\n\n### Step 2: Plan the Pipeline\n1. **Data Ingestion**: Identify what needs to be imported and how\n2. **Processing Steps**: Determine what prompts/transformations are needed\n3. **Output Goals**: Define what final objects should be created\n\n### Step 3: Execute the Solution\n- Set up the AI prompts using '/setup_ai_prompt'\n- Apply the AI prompts to input text using '/apply_prompt_to_data'\n- Chain multiple processing steps if needed\n- Verify results using the management endpoints\n\n## Advanced Patterns\n\n### Iterative Processing\nBuild complex workflows by chaining operations:\n1. Ingest raw data\n2. Extract key points\n3. Categorize points\n4. Generate final report\n\n## Example Problem-Solving Scenarios\n\n### Competitive Intelligence\n**User**: \"Compare our product features with 5 competitor websites\"\n**Solution**:\n1. Use '/get_data_from_url' to scrape competitor sites\n2. Use '/setup_ai_prompt' to prepare a prompt that extracts product info\n3. Use '/apply_prompt_to_data' to extract features from each web page's data\n4. Compare the resulting data\n\n### Document Processing\n**User**: \"Extract action items from 20 meeting transcripts and categorize them\"\n**Solution**:\n1. Upload transcripts and process them locally\n2. Use '/setup_ai_prompt' to prepare a prompt that extracts action items\n3. Use '/setup_ai_prompt' to prepare a prompt that categorizes action items\n3. Use '/apply_prompt_to_data' with 'structured' return_type\n4. Iterate through each output and use '/apply_prompt_to_data' to categorize each item\n\n## Best Practices\n\n### Naming Conventions\n- Use descriptive object names: 'meeting_transcripts', 'competitor_analysis'\n- Include processing step in names: 'raw_articles' â 'article_summaries' â 'final_report'\n\n### Prompt Engineering\n- Use clear placeholders: '{object_name}'\n- Anywhere an object name is needed, use a single consistent name instead of appending UIDs or timestamps.\n- Specify desired output format in prompts\n- Include example outputs when helpful\n- Request structured data when building pipelines\n\n### Efficiency\n- Use existing capabilities as they are rather than rewriting them\n- All error messages displayed in the app should include the direct output from the API function being called.\n- Ensure you set up any AI prompts before you use them.\n- If PDF parsing is necessary, add it using the client-side library pdfjs-dist, and the line: import pdfjsWorker from 'pdfjs-dist/build/pdf.worker?url'\n- If plots and charts are necessary, add them using Chart.js with react-chartjs-2.\n- Combine related processing steps when possible\n\n## Additional functions and APIs:\n\n\n## Running the app\n\nGive the code one more quick check.  If you have not done so yet, run the necessary commands by calling the `shell_exec` tool (e.g., install dependencies with `npm install`, run tests, or invoke curl to exercise the builder APIs). When you need to create or update files, call `fs_write` with the full file content.always use the registered tools for filesystem and shell access.\n\nAlso, create any text-processing prompts that you will need by issuing curl calls (via `shell_exec` + curl) to the `/setup_ai_prompt` endpoint with the prompts this app will use. Provide the relevant commands and summarize the outputs so the user knows the APIs were exercised.\n\nFrom here onward, ensure that every code or command change goes through the appropriate tool call so it actually takes effect.\n\n## Mocking Third-Party APIs\n\nFor all third-party APIs that are mentioned in the user instructions and are not listed in the API endpoints provided, please mock them. For example, if the user mentioned integrating a third party such as ElevenLabs or Fish AI, and there are no explicit instructions on how to integrate them, mock them using reasonable assumptions of the API structure.\n\n## Design Requirements\n\nIf the user requirements did not define styles, change the style of the app using the set of instructions below that are most relevant to the user task:\n\n(For screens where users upload documents, extract structured data, and view outputs):\nGenerate a three-step Upload & Extract flow for seed-to-Series-B small-business brands.\nâ¢ Step 1: File upload card with drag-and-drop and âChoose Fileâ button, branded with some PRIMARY_COLOR of your choice.\nâ¢ Step 2: Extraction progress screen showing a Tailwind-styled spinner, clear status message, and cancel option.\nâ¢ Step 3: Data output table with Tailwind table classes, sortable columns, and âDownload CSVâ button.\nâ¢ Step 4: Ensure mobile breakpoints collapse sidebars into accordion panels; implement dark mode variants; include ARIA labels on all interactive elements.â\n\n(For embedding a conversational AI widget into an existing portal):\nProduce a chatbot UI panel that sits at bottom-right:\nâ¢ Step 1:  Minimal header bar with logo icon and âHelpâ label in some PRIMARY_COLOR of your choice.\nâ¢ Step 2:  Scrollable message window styled with alternating light/dark bubble backgrounds.\nâ¢ Step 3:  Input area with accessible placeholder text 'Ask me anything...', a send icon button, and an optional attachment button.\nâ¢ Step 4:  Ensure focus outlines, keyboard navigation, and proper aria-live regions for new messages.\nâ¢ Step 5:  Include a mobile view where the panel expands full-screen and a dark-mode toggle.â\n\n(For workflows where users upload a document, enter a prompt themselves, then see a summary or output):\nDesign a three-column layout on desktop, single-column on mobile, for Upload + Prompt + Summary:\nâ¢ Step 1:  Upload Column: Drag-and-drop zone with dashed border and an upload progress bar.\nâ¢ Step 2:  Prompt Column: A text area with placeholder 'Enter instructions...', character count, and 'Run' button in green.\nâ¢ Step 3:  Summary Column: Shows AI-generated summary in a scrollable card with expandable sections.\nâ¢ Step 4:  Follow minimal luxe style: plenty of white space, 2xl rounded corners, soft shadows. Use Tailwind classes for spacing. Include dark-mode card variants. Add aria-describedby on summary sections.â\n\n## Secrets Vault Requirements\n\n\nSECRET VAULT CONNECTION INFORMATION\n==============================\nYou already have access to a server-side encrypted storage for API keys, passwords, and other secrets. Secrets persist between server restarts and are never exposed to the browser. Use this to hold user secrets or other keys.\n\nThis is different from the API calls - You already wrote the underlying code, and the files are presented as truncated for the sake of space.  Do not attempt to rewrite these files or re-create them elsewhere..\n\nTo use this secrets vault, all you have to do is the following:\ncd server\nnpm install express\n\nThen, add this import to your code and use the following local function structure:\n\nimport { useSecrets } from './hooks/useSecrets';\n\nfunction MyComponent() {\n  const { getSecret, setSecret } = useSecrets();\n\n  // Get a secret\n  const var1 = await getSecret('my_variable_1');\n\n  // Set a secret\n  await setSecret('database_password', 'my-password');\n\n  return <div>Variable 1: {var1}</div>;\n}\n\nSecrets set via this method will persist and can be used across runs of the app. Just use the existing files and call the vault this way.\n\n",
  "raw_message_parts": null
}